what is performance?
1.measure of how fast the system is if we keep below paremeter constants:
 1. workload.
     -> amount of backend data
     ->Request volume.
 2.Hardware assigned to system:
   RAM
   CPU
   Memory

Goal is to design a system whose:
->performace does not severely degrade when workload is increases.
->performance should increase if we give superior hardware.

how performance problems occur?
->when the rate of feeding the request to a system is more than the rate of consumption, these requests are queued up somewhere.
  reaspns:
  ->inefficient code.
  -> serial code execution, ex synchronised keyword in java which allows one thread at a time.
  -> limited resource capacity.even if u have the capability to execute multiple requests paralelly, but you don't have lot of CPUs.(have the capability to build multiple road
      but you don't have land space)

principles to ensure high performace:
1. Efficiency.
2. Concurrency
3. capcity

there are single request processing and concurrent request processing.

Efficiency: how efficient a single request is proceesed by system.(suppose you have a CPU which has enough capacity to process this request), 
            does our algorithm utilises these resources efficiently.
            DB queries should be utilising as less resources as possible.
            DB design should be efficient, use of proper data structures.(indexing etc)
            resources efficiently.

Concurrency: how efficiently the operating system process multiple request present in the system at same time.
             when one request occupied all resources, other request won't get it and quieing will happeb

Latency vs throughput:
Latency: wait time + process time of a request.
throughput: how many request the system can process in a given time.
if latency is less, throughput will be more.

tail latency is an indication of queuing in system, 

minimizing network latency:
1. using connection pool.
2. use persistent connection, browser by default use persistent connection. for interservice communication if u do a call using
   axios, u need to specify the connection:keep-alive because internally axios uses https library of node.js which by default doesn't
   use persistent connections.
3. for intraservuce communication use, grpc(binary protocol) protocol instead of https(ASCII transfer).
4. compress the data which needs to be transferred, it can significantly reduce data transfer latency.

Memory access latency:
 1. every process in the app requires some memory, when the memeory is about to go out of bound, garbage collectors run very aggressively,
    this leads to lot of latency because GC algorithms blocks all the threads to clean up memory.
 2. if we assign too much heap memory, then also GC algotihm will take too much time to sweep through big area.
 3. database buffer memory, while performing write operations, data is first fetched from disk into database buffer memory and the change
    happens there, then this new changed data is again written back to disk. so if the buffer memory is small, memory access 
latency will come.

problem of memory bloat:
 code base should be small.(each instruction leads toc ontext switch between RAM and processor).
 weak references, java has this concept where if memory is running short, GC can remove objects, and weakly referred objects are first
 checked if they really exist or not, this check is responsibility of program code.


compute over storage:
the lesser the data in disk, the lesser will be loaded in memmory.
normalisation is a means to avoid duplicate data, a way to store as much info as possible in as less disk space as possible.
data can be computer using queries at run time, we should never store it.

Disk Latency:
whenever we write data on disk, context switching happens. which adds to disk latency.
ex: logging, having multiple console.log statements slows down the app a lot because of context switching,

asynchronous logging:main thread instead of logging himself and waiting till logging is complete and letting cpu in idle state,
it put's these logging into a queue, which is picked up by seperate thread. so our CPU is never idle,downside is if the app crashes.
some of the logging statement might miss leading to info lost.

reverse proxy can be used to cache data, mostly NGNIX does it.


denormalisation vs normalisation:
1. when application is write heavy, ensure data in normalised because more info we can save in less space, so when we want to perform
writes, less data will be loaded in database buffer memory, contrary to that. if the database buffer memory is big and aplication is read
heavy, go for denormalisation because lesser joins means faster lookups.
 
  
