what is performance?
1.measure of how fast the system is if we keep below paremeter constants:
 1. workload.
     -> amount of backend data
     ->Request volume.
 2.Hardware assigned to system:
   RAM
   CPU
   Memory

Goal is to design a system whose:
->performace does not severely degrade when workload is increases.
->performance should increase if we give superior hardware.

how performance problems occur?
->when the rate of feeding the request to a system is more than the rate of consumption, these requests are queued up somewhere.
  reaspns:
  ->inefficient code.
  -> serial code execution, ex synchronised keyword in java which allows one thread at a time.
  -> limited resource capacity.even if u have the capability to execute multiple requests paralelly, but you don't have lot of CPUs.(have the capability to build multiple road
      but you don't have land space)

principles to ensure high performace:
1. Efficiency.
2. Concurrency
3. capcity

there are single request processing and concurrent request processing.

Efficiency: how efficient a single request is proceesed by system.(suppose you have a CPU which has enough capacity to process this request), 
            does our algorithm utilises these resources efficiently.
            DB queries should be utilising as less resources as possible.
            DB design should be efficient, use of proper data structures.(indexing etc)
            resources efficiently.

Concurrency: how efficiently the operating system process multiple request present in the system at same time.
             when one request occupied all resources, other request won't get it and quieing will happeb

Latency vs throughput:
Latency: wait time + process time of a request.
throughput: how many request the system can process in a given time.
if latency is less, throughput will be more.

tail latency is an indication of queuing in system, 

minimizing network latency:
1. using connection pool.
2. use persistent connection, browser by default use persistent connection. for interservice communication if u do a call using
   axios, u need to specify the connection:keep-alive because internally axios uses https library of node.js which by default doesn't
   use persistent connections.
3. for intraservuce communication use, grpc(binary protocol) protocol instead of https(ASCII transfer).
4. compress the data which needs to be transferred, it can significantly reduce data transfer latency.

Memory access latency:
 1. every process in the app requires some memory, when the memeory is about to go out of bound, garbage collectors run very aggressively,
    this leads to lot of latency because GC algorithms blocks all the threads to clean up memory.
 2. if we assign too much heap memory, then also GC algotihm will take too much time to sweep through big area.
 3. database buffer memory, while performing write operations, data is first fetched from disk into database buffer memory and the change
    happens there, then this new changed data is again written back to disk. so if the buffer memory is small, memory access 
latency will come.

problem of memory bloat:
 code base should be small.(each instruction leads toc ontext switch between RAM and processor).
 weak references, java has this concept where if memory is running short, GC can remove objects, and weakly referred objects are first
 checked if they really exist or not, this check is responsibility of program code.


compute over storage:
the lesser the data in disk, the lesser will be loaded in memmory.
normalisation is a means to avoid duplicate data, a way to store as much info as possible in as less disk space as possible.
data can be computer using queries at run time, we should never store it.

Disk Latency:
whenever we write data on disk, context switching happens. which adds to disk latency.
ex: logging, having multiple console.log statements slows down the app a lot because of context switching,

asynchronous logging:main thread instead of logging himself and waiting till logging is complete and letting cpu in idle state,
it put's these logging into a queue, which is picked up by seperate thread. so our CPU is never idle,downside is if the app crashes.
some of the logging statement might miss leading to info lost.

reverse proxy can be used to cache data, mostly NGNIX does it.


denormalisation vs normalisation:
1. when application is write heavy, ensure data in normalised because more info we can save in less space, so when we want to perform
writes, less data will be loaded in database buffer memory, contrary to that. if the database buffer memory is big and aplication is read
heavy, go for denormalisation because lesser joins means faster lookups.

In a database, table data is written to disk pages (which may not be sequential). An index (e.g., a B+Tree) stores pointers to those disk locations.

Reads become faster because the index lets the DB jump directly to the right disk page instead of scanning the whole table.

Writes become slower because, after writing the row to disk, the database must also update the index structure with the new row‚Äôs location (and potentially rebalance the tree).


subqueries are slower than joins.

CPU latency:
it happens due to context switching:
suppose mulitple threads are inside your system.right now thread 1 is in the memory and CPU is assigned to it.
suppose thread1 goes for some I/O operation it no longer needs CPU, now this process is evicted from memory and stored in process control block.and 
thread2 is loaded in memory and cpu is assigned to it.this eviction and loading of new process in memory takes some time.

methods to avoid :
batch I/O:thread will not start processing I/O immediately , it will push the I/O into a queue and then process the batch together,
so no of context switch will decrease.
for I/O intensive tasks, go for Node.js. for CPU intensive task, go for single thread.

Multi-threaded scheduling (classic OS, multiple threads)

Policeman = CPU core

Road = thread

If one road is blocked (I/O wait), the policeman leaves and goes to another road.

This is how the OS does context switching to keep CPUs busy.

üîπ Event loop (async I/O, single-threaded model like Node.js)

Only 1 policeman, 1 active road (the main loop).

When a car (task) gets blocked at a railway crossing (I/O), the policeman doesn‚Äôt walk away.

Instead, he keeps letting cars from other roads merge into this one road (tasks queued in the event loop).

Meanwhile, a railway signal system (the event loop‚Äôs I/O watcher) keeps checking: ‚ÄúIs the crossing open yet?‚Äù

When it opens, the waiting cars from that blocked road are allowed back into the flow.


virtualisation call also reduce cpu latency, how?


Amdahl's law: it predicts the max oncurrency our system can have based on the single portion %, single portion occurs in any system
               because of locks etc. even if we put infinite processors, the concurrency can't go beyond this level.
Gunther's scalability law: along with queing which Amdahl's law deals with, it also takes into account Gunther's law.
              ex: if we have a volatile variable , it's value will be same across all threads, so when one thread modifies it,
                 the value has to be replicated into other threads processes, which leads to delay.(so no of request a system can
                process in given time decreases, hence throughput decreases).

contention in system(leading to queue):
1. CPU/DISK/Network latency:
    Disk: use RAID disk because they are sequential array of disk where your data is replicated in multiple positions,
          so mulitple threads if want to access same data on disk, it will be faster.for CPU vertical scaling can be done
    Threadpool size: if the threads are too much, lot of request will be assigned to system. but they won't be able to process
    because threads will wait for CPU .
    if the size is very less, they will go inside listen queue.

Minimizing lock contention:
1. reduce the duration of locks by removing unwanted code out of synchronized block.
2. Lock striping: concurrent hashmap, divide a big data structure into small units, take locks only on those units.
3. insteaf of having global lock for both reads/writes. use Read/Write Locks, Read locks means no writer can write while
   read lock is there though other read lock can happen, write lock means no reader can read when data is being written.
 
  
