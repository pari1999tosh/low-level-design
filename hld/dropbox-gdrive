functional requirement:
 - upload file
 - download file
 - sync file across devices, 

out of scope:
 - don't design blob storage, use s3- that's a seperate question.

non functional:
 - availability
 - consistency
 - availability > consistency , main purpose of such sevrices is to add arhcitecture diagram, database diagram etc.
 - low latency uploads and downloads(as low as possible)
 - support large file uploads like 50gb.
 - resume upload functionality should be there.
 - High data integrity.it's okay if syncing takes some time, but after syncing it should be accurate.

entities:
 - file binary data(raw data)
 - file metadata
 - users

APIs:
 - POST /files
    body: file(binary data)
 - GET /files
      download the files

 - GET /changes?since={timestamp}, this endpoint is used to know if something has changed


while downloading a file, we don't download from s3 in the service and then let browser download it from service.
instead we give presigned s3 url to browser and browser directly downloads it.


after 30:00 video:
 - main problems in current design:
   API gateways and browser etc has hard limit on payload size of POST request (10 mb), so we can't upload large files.
  so we need a mechanism to directly upload in s3, so we just send metadata of file with mimetype,size etc via API gateway
  and request s3 to give us a presigned URL, now this presigned url is returned to browser and via browser we directly upload file
  presigned URL has digital signature and expirty time.

- it will still take lot of time.if internet connection is lost, we will have to start again. so we do chunking.
  we will store chunks array in file metadata while uploading,each chunk will have it's own s3 link and id,status
- in case of resume upload, we get chunks from metadata db and then compare with the chunks in laptop file system and upload only
  those which are not yet uploaded. for comparition we will use fingerprinting.
  we calculate the hash of each chunks bytes and these can be used as unique IDs in file metadata.
