functional requirement:
 - upload file
 - download file
 - sync file across devices, 

out of scope:
 - don't design blob storage, use s3- that's a seperate question.

non functional:
 - availability
 - consistency
 - availability > consistency , main purpose of such sevrices is to add arhcitecture diagram, database diagram etc.
 - low latency uploads and downloads(as low as possible)
 - support large file uploads like 50gb.
 - resume upload functionality should be there.
 - High data integrity.it's okay if syncing takes some time, but after syncing it should be accurate.

entities:
 - file binary data(raw data)
 - file metadata
 - users

APIs:
 - POST /files
    body: file(binary data)
 - GET /files
      download the files

 - GET /changes?since={timestamp}, this endpoint is used to know if something has changed


while downloading a file, we don't download from s3 in the service and then let browser download it from service.
instead we give presigned s3 url to browser and browser directly downloads it.


after 30:00 video:
 - main problems in current design:
   API gateways and browser etc has hard limit on payload size of POST request (10 mb), so we can't upload large files.
  so we need a mechanism to directly upload in s3, so we just send metadata of file with mimetype,size etc via API gateway
  and request s3 to give us a presigned URL, now this presigned url is returned to browser and via browser we directly upload file
  presigned URL has digital signature and expirty time.

- it will still take lot of time.if internet connection is lost, we will have to start again. so we do chunking.
  we will store chunks array in file metadata while uploading,each chunk will have it's own s3 link and id,status
- in case of resume upload, we get chunks from metadata db and then compare with the chunks in laptop file system and upload only
  those which are not yet uploaded. for comparition we will use fingerprinting.
  we calculate the hash of each chunks bytes and these can be used as unique IDs in file metadata.
  instead of using hash in browser and then send it to file metadata, we could have generated IDs in metadata and store in laptop
  filesystem, but in case when we delete the file from laptop and download again, these ids will be lost, but in case of hash
  the same hash will be generated from the chunks bytes.
- another problem is , when chunks are started uploading from browser to s3 and the status is updated in db, hacker/stupid user can tweak
  with browser and hamper the network calls and so inconsistent status he will send for a particular chunk , then he will create
  problem for engineering team and engineering team doesn't have access to client desktop , it will be very difficult to debug.
